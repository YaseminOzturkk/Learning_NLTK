{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNy7NsX3imikcgwlxdGjj0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YaseminOzturkk/Learning_NLTK/blob/main/NLTK_Exercises1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "luM-1-bPphpq",
        "outputId": "75dc88fc-d146-4dd7-d10d-c1b6f6b3a159"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-08422b201ac9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install nltk\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#Download the NLTK library.\n",
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the NLTK library.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C18LoWtpr0D",
        "outputId": "c769fd66-1f63-45ab-b7c1-aa3a8e2b9b42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We use the tokenize module in nltk for the operations we want to import the library and do. The word_tokenize() module tells us to separate each word in a sentence, while the sent_tokenize() module is only used to separate two sentences.\n",
        "sentence= \"Text Analysis is a type of data analysis in which the content and meaning of texts, as well as their structure and discourse, are thoroughly examined and classified into meaningful components. This type of analysis has become increasingly critical for businesses in the past decade due to its efficiency in extracting valuable customer information.\"\n",
        "print(sent_tokenize(sentence))\n",
        "#We split the sentence into two."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FakFI0X8pz1f",
        "outputId": "81745342-f2be-4aad-d0c0-01cb08490fb3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Text Analysis is a type of data analysis in which the content and meaning of texts, as well as their structure and discourse, are thoroughly examined and classified into meaningful components.', 'This type of analysis has become increasingly critical for businesses in the past decade due to its efficiency in extracting valuable customer information.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we have separated each word in a sentence from each other. This is called tokenization process, while preprocessing the data, other processes like this are applied.\n",
        "print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULHrdfHLrmnO",
        "outputId": "5d358410-6469-4d3e-d289-f03592202fec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Text', 'Analysis', 'is', 'a', 'type', 'of', 'data', 'analysis', 'in', 'which', 'the', 'content', 'and', 'meaning', 'of', 'texts', ',', 'as', 'well', 'as', 'their', 'structure', 'and', 'discourse', ',', 'are', 'thoroughly', 'examined', 'and', 'classified', 'into', 'meaningful', 'components', '.', 'This', 'type', 'of', 'analysis', 'has', 'become', 'increasingly', 'critical', 'for', 'businesses', 'in', 'the', 'past', 'decade', 'due', 'to', 'its', 'efficiency', 'in', 'extracting', 'valuable', 'customer', 'information', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, we can do the stemming process.\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "words = \"gaming, the gamers play games\"\n",
        "words = word_tokenize(words)\n",
        " \n",
        "for word in words:\n",
        "    print(word + \":\" + ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqeXKIA0r8-0",
        "outputId": "733c7f25-6cd3-4c20-b388-ca15f557e374"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gaming:game\n",
            ",:,\n",
            "the:the\n",
            "gamers:gamer\n",
            "play:play\n",
            "games:game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PorterStemmer does not support the above operation for Turkish. That's why we have to do it in English, for Turkish it will be enough to do from snowballstemmer import stemmer."
      ],
      "metadata": {
        "id": "omX_SWQVsa1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK also saves us from having to deal with unnecessary words when we are pre-processing a data set, that is, when we are going to make the data machine understandable. Actually, it's a little wrong to call it redundant, it just saves you from dealing with non-important words (i, me, my, myself, we, our, ours etc.) while doing attribute extraction. Let's try this opportunity that NLTK provides us, it also supports it in Turkish."
      ],
      "metadata": {
        "id": "5gKWD8yjtmLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To check the list of stopwords you can type the following commands in the python shell.\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba8xkAFBslnO",
        "outputId": "b6976a2c-9c19-4c38-802f-e2da15332c59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stop words with NLTK\n",
        "#The following program removes stop words from a piece of text:\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "  \n",
        "stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "word_tokens = word_tokenize(example_sent)\n",
        "  \n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  \n",
        "filtered_sentence = []\n",
        "  \n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "  \n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btLYln3KwU0q",
        "outputId": "9a617f01-f451-45d1-fab4-b1df6092b033"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    }
  ]
}